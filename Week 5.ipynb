{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.5.11\n",
      "  latest version: 4.8.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/jupyterlab/conda/envs/python\n",
      "\n",
      "  added / updated specs: \n",
      "    - geopy\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    certifi-2019.11.28         |           py36_0         149 KB  conda-forge\n",
      "    scikit-learn-0.20.1        |   py36h22eb022_0         5.7 MB\n",
      "    liblapack-3.8.0            |      11_openblas          10 KB  conda-forge\n",
      "    liblapacke-3.8.0           |      11_openblas          10 KB  conda-forge\n",
      "    geographiclib-1.50         |             py_0          34 KB  conda-forge\n",
      "    libopenblas-0.3.6          |       h5a2b251_2         7.7 MB\n",
      "    numpy-1.17.3               |   py36h95a1406_0         5.2 MB  conda-forge\n",
      "    scipy-1.4.1                |   py36h921218d_0        18.9 MB  conda-forge\n",
      "    libcblas-3.8.0             |      11_openblas          10 KB  conda-forge\n",
      "    libblas-3.8.0              |      11_openblas          10 KB  conda-forge\n",
      "    geopy-1.20.0               |             py_0          57 KB  conda-forge\n",
      "    blas-2.11                  |         openblas          10 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        37.8 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "    geographiclib: 1.50-py_0                              conda-forge\n",
      "    geopy:         1.20.0-py_0                            conda-forge\n",
      "    libblas:       3.8.0-11_openblas                      conda-forge\n",
      "    libcblas:      3.8.0-11_openblas                      conda-forge\n",
      "    liblapack:     3.8.0-11_openblas                      conda-forge\n",
      "    liblapacke:    3.8.0-11_openblas                      conda-forge\n",
      "    libopenblas:   0.3.6-h5a2b251_2                                  \n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "    blas:          1.1-openblas                           conda-forge --> 2.11-openblas         conda-forge\n",
      "    certifi:       2019.9.11-py36_0                       conda-forge --> 2019.11.28-py36_0     conda-forge\n",
      "    numpy:         1.16.2-py36_blas_openblash1522bff_0    conda-forge [blas_openblas] --> 1.17.3-py36h95a1406_0 conda-forge\n",
      "    scipy:         1.2.1-py36_blas_openblash1522bff_0     conda-forge [blas_openblas] --> 1.4.1-py36h921218d_0  conda-forge\n",
      "\n",
      "The following packages will be DOWNGRADED:\n",
      "\n",
      "    scikit-learn:  0.20.1-py36_blas_openblashebff5e3_1200 conda-forge [blas_openblas] --> 0.20.1-py36h22eb022_0            \n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "certifi-2019.11.28   | 149 KB    | ##################################### | 100% \n",
      "scikit-learn-0.20.1  | 5.7 MB    | ##################################### | 100% \n",
      "liblapack-3.8.0      | 10 KB     | ##################################### | 100% \n",
      "liblapacke-3.8.0     | 10 KB     | ##################################### | 100% \n",
      "geographiclib-1.50   | 34 KB     | ##################################### | 100% \n",
      "libopenblas-0.3.6    | 7.7 MB    | ##################################### | 100% \n",
      "numpy-1.17.3         | 5.2 MB    | ##################################### | 100% \n",
      "scipy-1.4.1          | 18.9 MB   | ##################################### | 100% \n",
      "libcblas-3.8.0       | 10 KB     | ##################################### | 100% \n",
      "libblas-3.8.0        | 10 KB     | ##################################### | 100% \n",
      "geopy-1.20.0         | 57 KB     | ##################################### | 100% \n",
      "blas-2.11            | 10 KB     | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.5.11\n",
      "  latest version: 4.8.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.5.11\n",
      "  latest version: 4.8.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/jupyterlab/conda/envs/python\n",
      "\n",
      "  added / updated specs: \n",
      "    - beautifulsoup4\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    openssl-1.1.1              |       h7b6447c_0         5.0 MB  anaconda\n",
      "    certifi-2019.11.28         |           py36_0         156 KB  anaconda\n",
      "    beautifulsoup4-4.8.2       |           py36_0         161 KB  anaconda\n",
      "    soupsieve-1.9.5            |           py36_0          61 KB  anaconda\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         5.4 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "    soupsieve:      1.9.5-py36_0      anaconda   \n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "    beautifulsoup4: 4.6.3-py37_0                  --> 4.8.2-py36_0      anaconda\n",
      "    certifi:        2019.11.28-py36_0 conda-forge --> 2019.11.28-py36_0 anaconda\n",
      "    openssl:        1.1.1d-h516909a_0 conda-forge --> 1.1.1-h7b6447c_0  anaconda\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "openssl-1.1.1        | 5.0 MB    | ##################################### | 100% \n",
      "certifi-2019.11.28   | 156 KB    | ##################################### | 100% \n",
      "beautifulsoup4-4.8.2 | 161 KB    | ##################################### | 100% \n",
      "soupsieve-1.9.5      | 61 KB     | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "Libraries are imported.\n"
     ]
    }
   ],
   "source": [
    "# importing libraries\n",
    "import numpy as np # library to handle data in a vectorized manner\n",
    "import pandas as pd # library for data analsysis\n",
    "\n",
    "import requests # library to handle requests\n",
    "import json # library to handle JSON files\n",
    "from pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "!conda install -c conda-forge geopy --yes # uncomment this line if you haven't completed the Foursquare API lab\n",
    "import geopy.geocoders # convert an address into latitude and longitude values\n",
    "\n",
    "!conda install -c conda-forge folium=0.5.0 --yes # uncomment this line if you haven't completed the Foursquare API lab\n",
    "import folium # map rendering library\n",
    "!conda install -c anaconda beautifulsoup4 --yes\n",
    "from bs4 import BeautifulSoup\n",
    "print('Libraries are imported.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'toronto_base.csv' does not exist: b'toronto_base.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f561688691de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Loading the dataset which is about postal codes in Toronto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# This dataset was created in week 3.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_toronto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'toronto_base.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf_toronto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'toronto_base.csv' does not exist: b'toronto_base.csv'"
     ]
    }
   ],
   "source": [
    "# Loading the dataset which is about postal codes in Toronto\n",
    "# This dataset was created in week 3. \n",
    "df_toronto = pd.read_csv('toronto_base.csv')\n",
    "df_toronto.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the city Toronto, latitude and longtitude are manually extracted via google search\n",
    "toronto_latitude = 43.6932; toronto_longitude = -79.3832\n",
    "map_toronto = folium.Map(location = [toronto_latitude, toronto_longitude], zoom_start = 10.7)\n",
    "\n",
    "# add markers to map\n",
    "for lat, lng, borough, neighborhood in zip(df_toronto['Latitude'], df_toronto['Longitude'], df_toronto['Borough'], df_toronto['Neighbourhood']):\n",
    "    label = '{}, {}'.format(neighborhood, borough)\n",
    "    label = folium.Popup(label, parse_html=True)\n",
    "    folium.CircleMarker(\n",
    "        [lat, lng],\n",
    "        radius=5,\n",
    "        popup=label,\n",
    "        color='blue',\n",
    "        fill=True,\n",
    "        fill_color='#3186cc',\n",
    "        fill_opacity=0.7).add_to(map_toronto)  \n",
    "    \n",
    "\n",
    "map_toronto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_toronto['Borough'] == 'Scarborough'\n",
    "\n",
    "# selecting only neighborhoods regarding to \"Scarborough\" borough.\n",
    "scarborough_data = df_toronto[df_toronto['Borough'] == 'Scarborough']\n",
    "scarborough_data = scarborough_data.reset_index(drop=True).drop(columns = 'Unnamed: 0')\n",
    "scarborough_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "address_scar = 'Scarborough, Toronto'\n",
    "latitude_scar = 43.773077\n",
    "longitude_scar = -79.257774\n",
    "print('The geograpical coordinate of \"Scarborough\" are: {}, {}.'.format(latitude_scar, longitude_scar))\n",
    "\n",
    "map_Scarborough = folium.Map(location=[latitude_scar, longitude_scar], zoom_start=11.5)\n",
    "\n",
    "# add markers to map\n",
    "for lat, lng, label in zip(scarborough_data['Latitude'], scarborough_data['Longitude'], scarborough_data['Neighbourhood']):\n",
    "    label = folium.Popup(label, parse_html=True)\n",
    "    folium.CircleMarker(\n",
    "        [lat, lng],\n",
    "        radius = 10,\n",
    "        popup = label,\n",
    "        color ='blue',\n",
    "        fill = True,\n",
    "        fill_color = '#3186cc',\n",
    "        fill_opacity = 0.7).add_to(map_Scarborough)  \n",
    "    \n",
    "map_Scarborough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foursquare_crawler (postal_code_list, neighborhood_list, lat_list, lng_list, LIMIT = 500, radius = 1000):\n",
    "    result_ds = []\n",
    "    counter = 0\n",
    "    for postal_code, neighborhood, lat, lng in zip(postal_code_list, neighborhood_list, lat_list, lng_list):\n",
    "         \n",
    "        # create the API request URL\n",
    "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
    "            CLIENT_ID, CLIENT_SECRET, VERSION, \n",
    "            lat, lng, radius, LIMIT)\n",
    "            \n",
    "        # make the GET request\n",
    "        results = requests.get(url).json()[\"response\"]['groups'][0]['items']\n",
    "        tmp_dict = {}\n",
    "        tmp_dict['Postal Code'] = postal_code; tmp_dict['Neighborhood(s)'] = neighborhood; \n",
    "        tmp_dict['Latitude'] = lat; tmp_dict['Longitude'] = lng;\n",
    "        tmp_dict['Crawling_result'] = results;\n",
    "        result_ds.append(tmp_dict)\n",
    "        counter += 1\n",
    "        print('{}.'.format(counter))\n",
    "        print('Data is Obtained, for the Postal Code {} (and Neighborhoods {}) SUCCESSFULLY.'.format(postal_code, neighborhood))\n",
    "    return result_ds;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @hiddel_cell\n",
    "CLIENT_ID = '0UOOVVQUXZTSOJ5302UDTB1L500GIY0RCTXSTUJYXUIFGJXG' # your Foursquare ID\n",
    "CLIENT_SECRET = 'IBQQ0QYDBQCBCDZ3QBRSVC1JWKU0HGMIIMDVM4DOJW34RGAB' # your Foursquare Secret\n",
    "VERSION = '20180605' # Foursquare API version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Crawling different neighborhoods inside \"Scarborough\"')\n",
    "Scarborough_foursquare_dataset = foursquare_crawler(list(scarborough_data['Postcode']),\n",
    "                                                   list(scarborough_data['Neighbourhood']),\n",
    "                                                   list(scarborough_data['Latitude']),\n",
    "                                                   list(scarborough_data['Longitude']),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "with open(\"Scarborough_foursquare_dataset.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(Scarborough_foursquare_dataset, fp)\n",
    "print('Received Data from Internet is Saved to Computer.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Scarborough_foursquare_dataset.txt\", \"rb\") as fp:   # Unpickling\n",
    "    Scarborough_foursquare_dataset = pickle.load(fp)\n",
    "# print(type(Scarborough_foursquare_dataset))\n",
    "# Scarborough_foursquare_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is created to connect to the saved list which is the received database. It will extract each venue \n",
    "# for every neighborhood inside the database\n",
    "\n",
    "def get_venue_dataset(foursquare_dataset):\n",
    "    result_df = pd.DataFrame(columns = ['Postal Code', 'Neighborhood', \n",
    "                                           'Neighborhood Latitude', 'Neighborhood Longitude',\n",
    "                                          'Venue', 'Venue Summary', 'Venue Category', 'Distance'])\n",
    "    # print(result_df)\n",
    "    \n",
    "    for neigh_dict in foursquare_dataset:\n",
    "        postal_code = neigh_dict['Postal Code']; neigh = neigh_dict['Neighborhood(s)']\n",
    "        lat = neigh_dict['Latitude']; lng = neigh_dict['Longitude']\n",
    "        print('Number of Venuse in Coordination \"{}\" Posal Code and \"{}\" Negihborhood(s) is:'.format(postal_code, neigh))\n",
    "        print(len(neigh_dict['Crawling_result']))\n",
    "        \n",
    "        for venue_dict in neigh_dict['Crawling_result']:\n",
    "            summary = venue_dict['reasons']['items'][0]['summary']\n",
    "            name = venue_dict['venue']['name']\n",
    "            dist = venue_dict['venue']['location']['distance']\n",
    "            cat =  venue_dict['venue']['categories'][0]['name']\n",
    "            \n",
    "            \n",
    "            # print({'Postal Code': postal_code, 'Neighborhood': neigh, \n",
    "            #                   'Neighborhood Latitude': lat, 'Neighborhood Longitude':lng,\n",
    "            #                   'Venue': name, 'Venue Summary': summary, \n",
    "            #                   'Venue Category': cat, 'Distance': dist})\n",
    "            \n",
    "            result_df = result_df.append({'Postal Code': postal_code, 'Neighborhood': neigh, \n",
    "                              'Neighborhood Latitude': lat, 'Neighborhood Longitude':lng,\n",
    "                              'Venue': name, 'Venue Summary': summary, \n",
    "                              'Venue Category': cat, 'Distance': dist}, ignore_index = True)\n",
    "            # print(result_df)\n",
    "    \n",
    "    return(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scarborough_venues = get_venue_dataset(Scarborough_foursquare_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scarborough_venues.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scarborough_venues.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scarborough_venues.to_csv('scarborough_venues.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scarborough_venues = pd.read_csv('scarborough_venues.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh_list = list(scarborough_venues['Neighborhood'].unique())\n",
    "print('Number of Neighborhoods inside Scarborough:')\n",
    "print(len(neigh_list))\n",
    "print('List of Neighborhoods inside Scarborough:')\n",
    "neigh_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh_venue_summary = scarborough_venues.groupby('Neighborhood').count()\n",
    "neigh_venue_summary.drop(columns = ['Unnamed: 0']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('There are {} uniques categories.'.format(len(scarborough_venues['Venue Category'].unique())))\n",
    "\n",
    "print('Here is the list of different categories:')\n",
    "list(scarborough_venues['Venue Category'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just for fun and deeper understanding\n",
    "print(type(scarborough_venues[['Venue Category']]))\n",
    "\n",
    "print(type(scarborough_venues['Venue Category']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# one hot encoding\n",
    "scarborough_onehot = pd.get_dummies(data = scarborough_venues, drop_first  = False, \n",
    "                              prefix = \"\", prefix_sep = \"\", columns = ['Venue Category'])\n",
    "scarborough_onehot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_list_of_features = [\n",
    " \n",
    " 'Neighborhood',\n",
    " 'Neighborhood Latitude',\n",
    " 'Neighborhood Longitude',\n",
    "\n",
    " 'African Restaurant',\n",
    " 'American Restaurant',\n",
    " 'Asian Restaurant',\n",
    "\n",
    " \n",
    " 'BBQ Joint',\n",
    " \n",
    " 'Bakery',\n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " 'Breakfast Spot',\n",
    "\n",
    " 'Burger Joint',\n",
    " \n",
    " \n",
    " \n",
    " 'Cajun / Creole Restaurant',\n",
    " 'Cantonese Restaurant',\n",
    " 'Caribbean Restaurant',\n",
    " 'Chinese Restaurant',\n",
    " \n",
    " 'Diner',\n",
    "\n",
    "\n",
    " 'Fast Food Restaurant',\n",
    " 'Filipino Restaurant',\n",
    " 'Fish Market',\n",
    " 'Food & Drink Shop',\n",
    " 'Fried Chicken Joint',\n",
    " 'Fruit & Vegetable Store',\n",
    " \n",
    " 'Greek Restaurant',\n",
    " 'Grocery Store',\n",
    " \n",
    " 'Hakka Restaurant',\n",
    " \n",
    " 'Hong Kong Restaurant',\n",
    "\n",
    " 'Hotpot Restaurant',\n",
    " \n",
    " 'Indian Restaurant',\n",
    "\n",
    " 'Italian Restaurant',\n",
    " 'Japanese Restaurant',\n",
    " 'Korean Restaurant',\n",
    " 'Latin American Restaurant',\n",
    "\n",
    "\n",
    "\n",
    " 'Malay Restaurant',\n",
    " \n",
    " 'Mediterranean Restaurant',\n",
    " \n",
    " 'Mexican Restaurant',\n",
    " 'Middle Eastern Restaurant',\n",
    " \n",
    " 'Noodle House',\n",
    " \n",
    " 'Pizza Place',\n",
    " \n",
    " 'Restaurant',\n",
    " 'Sandwich Place',\n",
    " 'Seafood Restaurant',\n",
    " 'Shanghai Restaurant',\n",
    " \n",
    " 'Sushi Restaurant',\n",
    " 'Taiwanese Restaurant',\n",
    " \n",
    " 'Thai Restaurant',\n",
    " \n",
    " 'Vegetarian / Vegan Restaurant',\n",
    " \n",
    " 'Vietnamese Restaurant',\n",
    " 'Wings Joint']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Neighborhood Latitude', 'Neighborhood Longitude']).groupby(\n",
    "    'Neighborhood').sum()\n",
    "\n",
    "\n",
    "scarborough_onehot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_name_list = list(scarborough_onehot.columns)\n",
    "restaurant_list = []\n",
    "\n",
    "\n",
    "for counter, value in enumerate(feat_name_list):\n",
    "    if value.find('Restaurant') != (-1):\n",
    "        restaurant_list.append(value)\n",
    "        \n",
    "scarborough_onehot['Total Restaurants'] = scarborough_onehot[restaurant_list].sum(axis = 1)\n",
    "scarborough_onehot = scarborough_onehot.drop(columns = restaurant_list)\n",
    "\n",
    "\n",
    "feat_name_list = list(scarborough_onehot.columns)\n",
    "joint_list = []\n",
    "\n",
    "\n",
    "for counter, value in enumerate(feat_name_list):\n",
    "    if value.find('Joint') != (-1):\n",
    "        joint_list.append(value)\n",
    "        \n",
    "scarborough_onehot['Total Joints'] = scarborough_onehot[joint_list].sum(axis = 1)\n",
    "scarborough_onehot = scarborough_onehot.drop(columns = joint_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scarborough_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import k-means from clustering stage\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# run k-means clustering\n",
    "kmeans = KMeans(n_clusters = 5, random_state = 0).fit(scarborough_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_df = pd.DataFrame(kmeans.cluster_centers_)\n",
    "means_df.columns = scarborough_onehot.columns\n",
    "means_df.index = ['G1','G2','G3','G4','G5']\n",
    "means_df['Total Sum'] = means_df.sum(axis = 1)\n",
    "means_df.sort_values(axis = 0, by = ['Total Sum'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh_summary = pd.DataFrame([scar_ds.index, 1 + kmeans.labels_]).T\n",
    "neigh_summary.columns = ['Neighborhood', 'Group']\n",
    "neigh_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh_summary[neigh_summary['Group'] == 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_of_neigh = list(neigh_summary[neigh_summary['Group'] == 5]['Neighborhood'])[0]\n",
    "scarborough_venues[scarborough_venues['Neighborhood'] == name_of_neigh].iloc[0,1:5].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "neigh_summary[neigh_summary['Group'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "neigh_summary[neigh_summary['Group'] == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "name_of_neigh = list(neigh_summary[neigh_summary['Group'] == 4]['Neighborhood'])[0]\n",
    "scarborough_venues[scarborough_venues['Neighborhood'] == name_of_neigh].iloc[0,1:5].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
